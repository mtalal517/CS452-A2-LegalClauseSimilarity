{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3c3848",
   "metadata": {},
   "source": [
    "# Legal Clause Similarity — Baseline experiments\n",
    "\n",
    "This notebook implements two baseline models (1) BiLSTM siamese encoder and (2) BiLSTM + Attention siamese encoder to predict semantic similarity between legal clauses from the provided `archive/` CSV files.\n",
    "\n",
    "Assumptions: positive pairs are sampled from clauses in the same CSV (same category); negative pairs are sampled across different categories. No pre-trained transformers are used (per task constraint).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07baf958",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "Run the following (PowerShell) to install minimal dependencies and open the notebook:\n",
    "\n",
    "```powershell\n",
    "python -m pip install -r requirements.txt\n",
    "jupyter notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e672f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load CSV files from archive into a single DataFrame\n",
    "root = Path('archive')\n",
    "csv_files = sorted(root.glob('*.csv'))\n",
    "rows = []\n",
    "for f in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "    except Exception as e:\n",
    "        # skip unreadable files\n",
    "        continue\n",
    "    # heuristics: look for columns that contain clause text - common names: 'text', 'clause', 'clause_text'\n",
    "    text_col = None\n",
    "    for candidate in ['text','clause','clause_text','clauseText','clause_text_1','0']:\n",
    "        if candidate in df.columns:\n",
    "            text_col = candidate\n",
    "            break\n",
    "    if text_col is None:\n",
    "        # fallback: take first string-like column\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype == object:\n",
    "                text_col = c\n",
    "                break\n",
    "    if text_col is None:\n",
    "        continue\n",
    "    for _, r in df.iterrows():\n",
    "        text = str(r[text_col]) if not pd.isnull(r[text_col]) else ''\n",
    "        if len(text.strip()) == 0:\n",
    "            continue\n",
    "        rows.append({'text': text.strip(), 'category': f.stem})\n",
    "\n",
    "clauses_df = pd.DataFrame(rows)\n",
    "print('Loaded clauses:', len(clauses_df))\n",
    "clauses_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86574ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build pairs: positive = same category, negative = different category\n",
    "# We'll sample to create a balanced dataset of pairs.\n",
    "def build_pairs(df, max_pos_per_cat=500, max_neg=20000, seed=42):\n",
    "    random.seed(seed)\n",
    "    categories = df['category'].unique().tolist()\n",
    "    cat_to_texts = {c: df[df['category']==c]['text'].tolist() for c in categories}\n",
    "    pairs = []\n",
    "    # positive pairs\n",
    "    for c, texts in cat_to_texts.items():\n",
    "        n = len(texts)\n",
    "        if n < 2:\n",
    "            continue\n",
    "        samples = texts if max_pos_per_cat is None else random.sample(texts, min(len(texts), max_pos_per_cat))\n",
    "        # create random positive pairs from samples\n",
    "        for i in range(len(samples)):\n",
    "            for j in range(i+1, len(samples)):\n",
    "                pairs.append((samples[i], samples[j], 1))\n",
    "    # negative pairs: sample pairs across categories\n",
    "    all_texts = df['text'].tolist()\n",
    "    neg = set()\n",
    "    attempts = 0\n",
    "    while len(neg) < max_neg and attempts < max_neg*10:\n",
    "        a = random.choice(all_texts)\n",
    "        b = random.choice(all_texts)\n",
    "        attempts += 1\n",
    "        # ensure different categories\n",
    "        if a == b:\n",
    "            continue\n",
    "        ca = df[df['text']==a]['category'].iloc[0]\n",
    "        cb = df[df['text']==b]['category'].iloc[0]\n",
    "        if ca == cb:\n",
    "            continue\n",
    "        neg.add((a,b))\n",
    "    for a,b in neg:\n",
    "        pairs.append((a,b,0))\n",
    "    random.shuffle(pairs)\n",
    "    return pairs\n",
    "\n",
    "pairs = build_pairs(clauses_df, max_pos_per_cat=50, max_neg=5000)\n",
    "print('Total pairs:', len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c63718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Simple tokenizer/vocab builder (whitespace + basic cleanup)\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    toks = text.split()\n",
    "    return toks\n",
    "\n",
    "# build vocab\n",
    "all_text = [t for p in pairs for t in (p[0], p[1])]\n",
    "counter = Counter()\n",
    "for t in all_text:\n",
    "    counter.update(simple_tokenize(t))\n",
    "\n",
    "# keep top-k words\n",
    "vocab_size = 20000\n",
    "most_common = counter.most_common(vocab_size-2)\n",
    "itos = ['<pad>','<unk>'] + [w for w,_ in most_common]\n",
    "stoi = {w:i for i,w in enumerate(itos)}\n",
    "\n",
    "def encode(text, max_len=128):\n",
    "    toks = simple_tokenize(text)[:max_len]\n",
    "    ids = [stoi.get(t, 1) for t in toks]\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [0]*(max_len-len(ids))\n",
    "    return ids\n",
    "\n",
    "# quick sanity\n",
    "print('Vocab size:', len(itos))\n",
    "print('Example encode:', encode(all_text[0])[:16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3067f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) PyTorch Dataset for pairs\n",
    "class ClausePairsDataset(Dataset):\n",
    "    def __init__(self, pairs, max_len=128):\n",
    "        self.pairs = pairs\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        a,b,l = self.pairs[idx]\n",
    "        a_ids = torch.tensor(encode(a, self.max_len), dtype=torch.long)\n",
    "        b_ids = torch.tensor(encode(b, self.max_len), dtype=torch.long)\n",
    "        return a_ids, b_ids, torch.tensor(l, dtype=torch.float)\n",
    "\n",
    "# split pairs into train/val/test\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "train_pairs, val_pairs = train_test_split(train_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "train_ds = ClausePairsDataset(train_pairs)\n",
    "val_ds = ClausePairsDataset(val_pairs)\n",
    "test_ds = ClausePairsDataset(test_pairs)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "print(len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Models: Siamese BiLSTM encoder and BiLSTM+Attention encoder\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hid_dim=128, n_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, batch_first=True, bidirectional=True, dropout=dropout if n_layers>1 else 0.0)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "    def forward(self, x):\n",
    "        # x: (B, L)\n",
    "        e = self.emb(x)\n",
    "        out, _ = self.lstm(e) # (B, L, 2*hid)\n",
    "        # average pool over sequence length\n",
    "        out_t = out.transpose(1,2)\n",
    "        pooled = self.pool(out_t).squeeze(-1)\n",
    "        return pooled\n",
    "\n",
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hid_dim=128, n_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, batch_first=True, bidirectional=True, dropout=dropout if n_layers>1 else 0.0)\n",
    "        self.attn = nn.Linear(hid_dim*2, 1)\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        out, _ = self.lstm(e)\n",
    "        # out: (B, L, 2*hid)\n",
    "        scores = self.attn(out).squeeze(-1)\n",
    "        weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        pooled = (out * weights).sum(dim=1)\n",
    "        return pooled\n",
    "\n",
    "class SiameseSimilarity(nn.Module):\n",
    "    def __init__(self, encoder, emb_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        # classifier on absolute difference and elementwise product\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim*2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "    def forward(self, a, b):\n",
    "        ea = self.encoder(a)\n",
    "        eb = self.encoder(b)\n",
    "        feat = torch.cat([torch.abs(ea-eb), ea*eb], dim=1)\n",
    "        out = self.fc(feat).squeeze(-1)\n",
    "        return out, ea, eb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Training & evaluation utilities\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for a,b,l in loader:\n",
    "        a = a.to(device); b = b.to(device); l = l.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits,_,_ = model(a,b)\n",
    "        loss = criterion(logits, l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    ps = []\n",
    "    for a,b,l in loader:\n",
    "        a = a.to(device); b = b.to(device)\n",
    "        logits,_,_ = model(a,b)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.extend(l.numpy().tolist())\n",
    "        ps.extend(probs.tolist())\n",
    "    ys = np.array(ys)\n",
    "    ps = np.array(ps)\n",
    "    preds = (ps >= 0.5).astype(int)\n",
    "    acc = accuracy_score(ys, preds)\n",
    "    f1 = f1_score(ys, preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(ys, ps)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    return {'accuracy': acc, 'f1': f1, 'roc_auc': auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Small experiments: train both baselines for a few epochs\n",
    "vocab_n = len(itos)\n",
    "emb_dim = 128\n",
    "hid_dim = 128\n",
    "epochs = 3\n",
    "\n",
    "# Baseline A: BiLSTM\n",
    "encoder_a = BiLSTMEncoder(vocab_n, emb_dim=emb_dim, hid_dim=hid_dim).to(device)\n",
    "model_a = SiameseSimilarity(encoder_a, emb_dim=hid_dim*2).to(device)\n",
    "opt_a = torch.optim.Adam(model_a.parameters(), lr=1e-3)\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    loss = train_one_epoch(model_a, train_loader, opt_a, crit)\n",
    "    val_metrics = evaluate(model_a, val_loader)\n",
    "    print(f'BiLSTM Ep {ep+1}/{epochs} loss={loss:.4f} val={val_metrics}')\n",
    "\n",
    "# Baseline B: BiLSTM + Attention\n",
    "encoder_b = AttentionEncoder(vocab_n, emb_dim=emb_dim, hid_dim=hid_dim).to(device)\n",
    "model_b = SiameseSimilarity(encoder_b, emb_dim=hid_dim*2).to(device)\n",
    "opt_b = torch.optim.Adam(model_b.parameters(), lr=1e-3)\n",
    "for ep in range(epochs):\n",
    "    loss = train_one_epoch(model_b, train_loader, opt_b, crit)\n",
    "    val_metrics = evaluate(model_b, val_loader)\n",
    "    print(f'Attn Ep {ep+1}/{epochs} loss={loss:.4f} val={val_metrics}')\n",
    "\n",
    "# Final evaluation on test set\n",
    "print('Final BiLSTM test:', evaluate(model_a, test_loader))\n",
    "print('Final Attn test:', evaluate(model_b, test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84c18b",
   "metadata": {},
   "source": [
    "## Notes and Next Steps\n",
    "\n",
    "- This notebook implements two lightweight baselines suitable for quick experiments.\n",
    "- Limitations: random negative sampling (not hard negatives), small vocab and tokenization; no pre-trained semantic embeddings used (by constraint).\n",
    "- Improvements: add class-balanced sampling, curriculum/hard-negative mining, better tokenization (subword), and more epochs/hyperparameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional evaluation metrics and plotting (Precision, Recall, PR-AUC)\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score, roc_curve, precision_recall_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def evaluate_full(model, loader):\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for a,b,l in loader:\n",
    "            a = a.to(device); b = b.to(device)\n",
    "            logits,_,_ = model(a,b)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            ys.extend(l.numpy().tolist())\n",
    "            ps.extend(probs.tolist())\n",
    "    ys = np.array(ys); ps = np.array(ps)\n",
    "    preds = (ps >= 0.5).astype(int)\n",
    "    acc = accuracy_score(ys, preds)\n",
    "    prec = precision_score(ys, preds, zero_division=0)\n",
    "    rec = recall_score(ys, preds, zero_division=0)\n",
    "    f1m = f1_score(ys, preds, zero_division=0)\n",
    "    try:\n",
    "        rocauc = roc_auc_score(ys, ps)\n",
    "    except Exception:\n",
    "        rocauc = float('nan')\n",
    "    try:\n",
    "        pra = average_precision_score(ys, ps)\n",
    "    except Exception:\n",
    "        pra = float('nan')\n",
    "    metrics = {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1m, 'roc_auc': rocauc, 'pr_auc': pra}\n",
    "    return metrics, ys, ps\n",
    "\n",
    "\n",
    "def plot_roc_pr(ys, ps, title='Model'):\n",
    "    # ROC\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(ys, ps)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    except Exception:\n",
    "        fpr, tpr, roc_auc = None, None, float('nan')\n",
    "    # PR\n",
    "    try:\n",
    "        prec, rec, _ = precision_recall_curve(ys, ps)\n",
    "        pr_auc = auc(rec, prec)\n",
    "    except Exception:\n",
    "        prec, rec, pr_auc = None, None, float('nan')\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    if fpr is not None:\n",
    "        plt.plot(fpr, tpr, label=f'ROC AUC={roc_auc:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'ROC - {title}'); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    if rec is not None:\n",
    "        plt.plot(rec, prec, label=f'PR AUC={pr_auc:.3f}')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f'PR - {title}'); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    display(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d70444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training wrapper that logs per-epoch metrics and saves results\n",
    "from pathlib import Path\n",
    "\n",
    "def train_and_log(model, train_loader, val_loader, test_loader, epochs=5, lr=1e-3, name='model'):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    rows = []\n",
    "    for ep in range(1, epochs+1):\n",
    "        train_loss = train_one_epoch(model, train_loader, opt, crit)\n",
    "        val_metrics, _, _ = evaluate_full(model, val_loader)\n",
    "        print(f\"{name} Ep{ep}/{epochs} loss={train_loss:.4f} val={val_metrics}\")\n",
    "        row = {'epoch': ep, 'train_loss': train_loss}\n",
    "        for k,v in val_metrics.items():\n",
    "            row[f'val_{k}'] = v\n",
    "        rows.append(row)\n",
    "    # final test\n",
    "    test_metrics, ys, ps = evaluate_full(model, test_loader)\n",
    "    print(f\"{name} Test: {test_metrics}\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_dir = Path('outputs')\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    df.to_csv(out_dir / f'{name}_metrics_per_epoch.csv', index=False)\n",
    "    np.save(out_dir / f'{name}_test_ys.npy', ys)\n",
    "    np.save(out_dir / f'{name}_test_ps.npy', ps)\n",
    "    # plot ROC/PR\n",
    "    plot_roc_pr(ys, ps, title=name)\n",
    "    return df, test_metrics\n",
    "\n",
    "# Run training for both models and save logs (set epochs reasonably; adjust as needed)\n",
    "epochs_run = 5\n",
    "print('Starting training for BiLSTM...')\n",
    "df_a, test_a = train_and_log(model_a, train_loader, val_loader, test_loader, epochs=epochs_run, lr=1e-3, name='bilstm')\n",
    "print('\\nStarting training for Attention encoder...')\n",
    "df_b, test_b = train_and_log(model_b, train_loader, val_loader, test_loader, epochs=epochs_run, lr=1e-3, name='attn')\n",
    "\n",
    "# show a summary table\n",
    "summary = pd.DataFrame([{'model':'bilstm', **test_a}, {'model':'attn', **test_b}])\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a8f25",
   "metadata": {},
   "source": [
    "## Evaluation metrics definitions (categorical classification)\n",
    "\n",
    "- Accuracy: Measures how often the model correctly classifies clause pairs as “similar” or “different.” Use only if dataset is roughly balanced.  \n",
    "- Precision: Out of all predicted similar clause pairs, how many truly convey the same legal meaning? Important when false positives are costly.  \n",
    "- Recall: Out of all truly similar clauses, how many did the model identify? Important when missing a truly similar clause is costly.  \n",
    "- F1-Score: Harmonic mean of Precision and Recall — balances both. Standard metric for NLP classification tasks.  \n",
    "- ROC-AUC / PR-AUC: Evaluate the classifier’s ranking ability across thresholds. PR-AUC (average precision) is especially informative when the positive class is rare.\n",
    "\n",
    "The cells above compute these metrics, save per-epoch validation logs to `outputs/<model>_metrics_per_epoch.csv`, and save test set predictions to `outputs/<model>_test_ys.npy` and `outputs/<model>_test_ps.npy` for further analysis.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
